<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>BIFRÖST: 3D-Aware Image compositing with Language Instructions</title>
    <link href="./DreamBooth_files/style.css" rel="stylesheet">
    <script type="text/javascript" src="./DreamBooth_files/jquery.mlens-1.0.min.js"></script> 
    <script type="text/javascript" src="./DreamBooth_files/jquery.js"></script>
</head>

<body>
<div class="content">
  <h1><strong>BIFRÖST: 3D-Aware Image compositing with Language Instructions</strong></h1>
  <p id="authors"><span><a href="https://lingxiao-li.github.io/">Lingxiao Li</a><a href="https://kxgong.github.io/">Kaixiong Gong</a><a href="https://weihonglee.github.io/">Weihong Li</a><a href="https://delay-xili.github.io/">Xili Dai</a><a href="https://eetchen.github.io/">Tao Chen</a><a href="https://yuan-xiaojun.github.io/Yuan-Xiaojun/">Xiaojun Yuan</a><a href="https://xyue.io/">Xiangyu Yue</a></span><br>
    <span style="font-size: 24px">MMLab, CUHK & HKUST (GZ) & Fudan University & UESTC</span></p>
  <br>
  <img src="./DreamBooth_files/Figure_1.png" class="teaser-gif" style="width:100%;"><br>
  <h3 style="text-align:center">Bifröst results on various personalized image compositing tasks. Top: Bifröst is adept at precise, arbitrary object placement and replacement in a background image with a reference object and a language instruction, and achieves 3D-aware high-fidelity harmonized compositing results; Bottom Left: Given a coarse mask, Bifröst can change the pose of the object to follow the shape of the mask; Bottom Right: Our model adapts the identity of the reference image to the target image without changing the pose.</h3>
  <font size="+2">
          <p style="text-align: center;">
            <a href="https://arxiv.org/abs/2410.19079" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="DreamBooth_files/bibtex.txt" target="_blank">[BibTeX]</a>
          </p>
  </font>
</div>

<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>This paper introduces <em>Bifröst</em>, a novel 3D-aware framework that is built upon diffusion models to perform instruction-based image composition. Previous methods concentrate on image compositing at the 2D level, which fall short in handling complex spatial relationships (<em>e.g.</em>, occlusion). <em>Bifröst</em> addresses these issues by training MLLM as a 2.5D location predictor and integrating depth maps as an extra condition during the generation process to bridge the gap between 2D and 3D, which enhances spatial comprehension and supports sophisticated spatial interactions. Our method begins by fine-tuning MLLM with a custom counterfactual dataset to predict 2.5D object locations in complex backgrounds from language instructions. Then, the image-compositing model is uniquely designed to process multiple types of input features, enabling it to perform high-fidelity image compositions that consider occlusion, depth blur, and image harmonization. Extensive qualitative and quantitative evaluations demonstrate that <em>Bifröst</em> significantly outperforms existing methods, providing a robust solution for generating realistically composited images in scenarios demanding intricate spatial understanding. This work not only pushes the boundaries of generative image compositing but also reduces reliance on expensive annotated datasets by effectively utilizing existing resources in innovative ways.

</p>
</div>

<div class="content">
  <h2>Introduction</h2>
  <p>Achieving arbitrary personalized object-level image compositing necessitates a deep understanding of the visual concept inherent to both the identity of the reference object and spatial relations of the background image. To date, this task has not been well addressed. Paint-by-Example (Yang et al., 2023) and Objectstitch (Song et al., 2023) use a target image as the template to edit a specific region of the background image, but they could not generate ID-consistent contents, especially for untrained categories. On the other hand, (Chen et al., 2024; Song et al., 2024) generate objects with ID (identity) preserved in the target scene, but they fall short in processing complicated 3D geometry relations (e.g., the occlusion) as they only consider 2D-level composition. To sum up, previous methods mainly either 1) fail to achieve both ID preservation and background harmony, or 2) do not explicitly take into account the geometry behind the background and fail to accurately composite objects and backgrounds in complex spatial relations. </p>
  <p>We conclude that the root cause of the aforementioned issues is that image composition is conducted at a 2D level. Ideally, the composition operation should be done in a 3D space for precise 3D geometry relationships. However, accurately modeling a 3D scene with any given image, especially with only one view, is non-trivial and time-consuming (Liu et al., 2023b). To address these challenges, we introduce Bifröst†, which offers a 3D-aware framework for image composition without explicit 3D modeling. We achieve this by leveraging depth to indicate the 3D geometry relationship between the object and the background. In detail, our approach leverages a multi-modal large language model (MLLM) as a 2.5D location predictor (i.e., bounding box and depth for the object in the given background image). With the predicted bounding box and depth, our method yields a depth map for the composited image, which is fed into a diffusion model as guidance. This enables our method to achieve good ID preservation and background harmony simultaneously, as it is now aware of the spatial relations between them, and the conflict at the dimension of depth is eliminated. In addition, MLLM enables our method to composite images with text instructions, which enlarges the application scenario of Bifröst. Bifröst achieves significantly better visual results than the previous method, which in turn validates our conclusion. </p>
  <p>Our main contributions can be summarized as follows: 1) We are the first to embed depth into the image composition pipeline, which improves the ID preservation and background harmony simultaneously. 2) We delicately build a counterfactual dataset and fine-tuned MLLM as a powerful tool to predict 2.5D location of the object in a given background image. Further, the fine-tuned MLLM enables our approach to understand language instructions for image composition. 3) Our approach has demonstrated exceptional performance through comprehensive qualitative assessments and quantitative analyses on image compositing and outperforms other methods, Bifröst allows us to generate images with better control of occlusion, depth blur, and image harmonization. </p>  
    <br>
  <img class="summary-img" src="./DreamBooth_files/comparison.png" style="width:100%;"> <br>
</div>

<div class="content">
  <h2>Approach</h2>
  <p>In order to train one model under the SAR framework, one should first specify the hyper-parameters, sequence order and output intervals. Based on the order setting, we first rearrange the sequence to the causal version. And we set the target as the rearranged causal sequence. Next, based on the output intervals, we drop the last set of the rearranged sequence and prepend a class token. The resulting sequence is then fed into the proposed Fully Masked Transformer. Then the model can be trained with the common cross entropy loss.</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/model.png" style="width:100%;"> <br>
    <br>
  <img class="summary-img" src="./DreamBooth_files/algorithm.png" style="width:100%;"> <br>
</div>

<div class="content">
  <h2>Few-step Generation</h2>
  <p>We train a T2I model, Lumina-SAR, in the transition states of SAR. Its first feature is few-step inference. Lumina-SAR begins to produce meaningful images at around 4 to 8 steps. With 64 steps, it can deliver high-quality outputs, requiring a processing time of only less than 3s on one A100 GPU. Typically, the full 4096 steps (AR) take >60 times longer than that required for 64 steps.</p>
  <img class="summary-img" src="./DreamBooth_files/t2i_step.png" style="width:100%;">
</div>

<div class="content">
  <h2>Zero-shot Painting</h2>
  <p>Another advantage of SAR is the flexibility in inference order, which facilitates image editing tasks such as image inpainting and outpainting. We perform zero-shot painting with Lumina-SAR, where the mask can be any shape.</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/painting.png" style="width:100%;"> <br>
</div>

<div class="content">
  <h2>More Visualizations</h2>
    <p>
        <h3>Class-to-image on ImageNet</h3>
    </p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/c2i.png" style="width:100%;"> <br>
    <p>
        <h3>Text-to-image</h3>
    </p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/add_t2i.png" style="width:100%;"> <br>
</div>

<div class="content">
  <h2>BibTex</h2>
  <code>@INPROCEEDINGS{Li24,<br>
  &nbsp;&nbsp;title	= {BIFRÖST: 3D-Aware Image compositing with Language Instructions},<br>
  &nbsp;&nbsp;author	= {Lingxiao Li and Kaixiong Gong and Weihong Li and Xili Dai and Tao Chen and Xiaojun Yuan and Xiangyu Yue},<br>
  &nbsp;&nbsp;booktitle={Advanced Neural Information Processing System (NeurIPS)},<br>
  &nbsp;&nbsp;year={2024}<br>
  }</code>
</div>

<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>: We thank <a href="https://dreambooth.github.io">DreamBooth</a> for the page templates.</p>
</div>
</body>
</html>
